/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
Namespace(dataset='pepfunc', repeat=1, num_workers=0, amp=True, compile=True, batch_size=5, testbatch_size=5, epochs=80, wd=0.1, lr=0.001, beta=0.95, minlr=0.0, K=0.0, gradclipnorm=1.0, decompnoise=1e-06, seedoffset=0, warmstart=16, conststep=32, cosstep=32, use_y_scale=False, dp=0.0, eldp=0.0, act='silu', lossparam=0.0, advloss=False, embdp=0.0, embbn=False, emborthoinit=False, degreeemb=False, embln=True, featdim=-1, hiddim=64, caldim=-1, normA=False, laplacian=True, sqrtlambda=True, elres=True, usesvmix=True, vmean=True, vnorm=True, elvmean=True, elvnorm=True, snorm=True, gsizenorm=1.85, l_encoder='deepset', l_layers=3, l_combine='mul', l_aggr='mean', l_res=True, l_mlptailact1=True, l_mlplayers1=2, l_mlpnorm1='ln', l_mlptailact2=False, l_mlplayers2=0, l_mlpnorm2='none', num_layers=12, sv_uselinv=True, sv_tailact=True, sv_res=True, sv_numlayer=1, sv_norm='none', el_uselinv=True, el_uselins=False, el_tailact=True, el_numlayer=2, el_norm='none', el_uses=False, conv_uselinv=True, conv_tailact=True, conv_numlayer=1, conv_norm='none', predlin_numlayer=1, predlin_norm='none', lexp='mlp', lexp_layer=2, lexp_norm='ln', outln=False, pool='mean', Tm=1, save=None, load=None, use_pos=False, align_size=32)
fixed bincls
10873 2331 2331
split 10873 2331 2331
num_task 10
PiOModel(
  (inputencoder): QInputEncoder(
    (xemb): MultiEmbedding(
      (embedding_list): ModuleList(
        (0): Embedding(18, 64, padding_idx=0)
        (1): Embedding(4, 64, padding_idx=0)
        (2-3): 2 x Embedding(8, 64, padding_idx=0)
        (4): Embedding(6, 64, padding_idx=0)
        (5): Embedding(2, 64, padding_idx=0)
        (6): Embedding(7, 64, padding_idx=0)
        (7-8): 2 x Embedding(3, 64, padding_idx=0)
      )
      (postemb): Sequential(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=False)
      )
    )
    (edgeEmb): MultiEmbedding(
      (embedding_list): ModuleList(
        (0-2): 3 x Embedding(5, 64, padding_idx=0)
      )
      (postemb): Sequential(
        (0): LayerNorm((64,), eps=1e-05, elementwise_affine=False)
      )
    )
    (LambdaEmb): MLPEncoding(
      (lin): MLP(
        (lin): Sequential(
          (0): Linear(in_features=1, out_features=128, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
    (distEmb): MLPEncoding(
      (lin): MLP(
        (lin): Sequential(
          (0): Linear(in_features=1, out_features=128, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
  )
  (LambdaEncoder): PermEquiLayer(
    (set2set): Sequential(
      (0): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=64, out_features=64, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=64, out_features=64, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (1): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=64, out_features=64, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=64, out_features=64, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (2): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=64, out_features=64, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=64, out_features=64, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (3): Identity()
    )
    (set2vec): Sequential(
      (0): MLP(
      (lin): Sequential(
        (0): NoneNorm()
      )
    )
    )
  )
  (elprojs): ModuleList(
    (0-11): 12 x sv2el(
      (linv1): Linear(in_features=64, out_features=64, bias=False)
      (linv2): Linear(in_features=64, out_features=64, bias=False)
      (lins1): Identity()
      (lins2): Identity()
      (lin): MLP(
        (lin): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
          (3): Linear(in_features=64, out_features=64, bias=True)
          (4): NoneNorm()
          (5): SiLU(inplace=True)
        )
      )
    )
  )
  (svmixs): ModuleList(
    (0-11): 12 x svMix(
      (linv1): Linear(in_features=64, out_features=64, bias=False)
      (linv2): Linear(in_features=64, out_features=64, bias=False)
      (linv3): Identity()
      (lins1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (lins2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (lins3): Identity()
    )
  )
  (convs): ModuleList(
    (0-11): 12 x DirCFConv(
      (lins): MLP(
        (lin): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (linv): Linear(in_features=64, out_features=64, bias=False)
    )
  )
  (predlin): MLP(
    (lin): Sequential(
      (0): Linear(in_features=64, out_features=10, bias=True)
    )
  )
  (predln): Identity()
  (vln): Sequential(
    (0): VMean()
    (1): VNorm()
  )
  (elvln): Sequential(
    (0): VMean()
    (1): VNorm()
  )
  (sln): LayerNorm((64,), eps=1e-05, elementwise_affine=False)
)
numel 569994
Epoch 1 train time : 1177.1 loss: 4.05e-01
 test time : 773.0 Train 0.0 Validation 0.303316205739975 Test 0.30572736263275146
GPU memory 21.65
Epoch 2 train time : 596.5 loss: 3.20e-01
 test time : 75.9 Train 0.0 Validation 0.33648917078971863 Test 0.34881049394607544
Epoch 3 train time : 459.4 loss: 3.07e-01
 test time : 75.9 Train 0.0 Validation 0.3951285481452942 Test 0.39098846912384033
Epoch 4 train time : 457.6 loss: 2.93e-01
 test time : 75.8 Train 0.0 Validation 0.39865976572036743 Test 0.39320477843284607
Epoch 5 train time : 461.7 loss: 2.89e-01
 test time : 75.9 Train 0.0 Validation 0.4079055190086365 Test 0.39465755224227905
Epoch 6 train time : 458.0 loss: 2.86e-01
 test time : 76.7 Train 0.0 Validation 0.39376476407051086 Test 0.3983856737613678
Epoch 7 train time : 458.4 loss: 2.80e-01
 test time : 75.8 Train 0.0 Validation 0.41514986753463745 Test 0.4012886583805084
Epoch 8 train time : 455.7 loss: 2.77e-01
 test time : 75.9 Train 0.0 Validation 0.42371121048927307 Test 0.41289347410202026
Epoch 9 train time : 458.8 loss: 2.73e-01
 test time : 75.9 Train 0.0 Validation 0.43279606103897095 Test 0.41229957342147827
Epoch 10 train time : 458.6 loss: 2.67e-01
 test time : 75.9 Train 0.0 Validation 0.4624742567539215 Test 0.443198025226593
Epoch 11 train time : 456.4 loss: 2.64e-01
 test time : 75.8 Train 0.0 Validation 0.4782117009162903 Test 0.45882996916770935
Epoch 12 train time : 458.3 loss: 2.61e-01
 test time : 75.8 Train 0.0 Validation 0.48047900199890137 Test 0.46833086013793945
Epoch 13 train time : 459.0 loss: 2.57e-01
 test time : 75.9 Train 0.0 Validation 0.49261674284935 Test 0.48136019706726074
Epoch 14 train time : 458.6 loss: 2.57e-01
 test time : 75.8 Train 0.0 Validation 0.4941345751285553 Test 0.4843015670776367
Epoch 15 train time : 457.2 loss: 2.56e-01
 test time : 75.9 Train 0.0 Validation 0.5070855617523193 Test 0.47726860642433167
Epoch 16 train time : 459.4 loss: 2.54e-01
 test time : 75.8 Train 0.0 Validation 0.5071524381637573 Test 0.4905843734741211
Epoch 17 train time : 457.9 loss: 2.53e-01
 test time : 75.9 Train 0.0 Validation 0.5091960430145264 Test 0.4923929274082184
Epoch 18 train time : 457.8 loss: 2.53e-01
 test time : 75.8 Train 0.0 Validation 0.51153564453125 Test 0.48885470628738403
Epoch 19 train time : 458.2 loss: 2.50e-01
 test time : 75.6 Train 0.0 Validation 0.47649040818214417 Test 0.44848984479904175
Epoch 20 train time : 458.0 loss: 2.49e-01
 test time : 75.8 Train 0.0 Validation 0.5049331188201904 Test 0.48993268609046936
Epoch 21 train time : 459.8 loss: 2.48e-01
 test time : 75.8 Train 0.0 Validation 0.49754244089126587 Test 0.47806745767593384
Epoch 22 train time : 459.3 loss: 2.48e-01
 test time : 75.9 Train 0.0 Validation 0.5369973182678223 Test 0.516401469707489
Epoch 23 train time : 461.3 loss: 2.45e-01
 test time : 75.8 Train 0.0 Validation 0.49322184920310974 Test 0.4765292704105377
Epoch 24 train time : 457.2 loss: 2.44e-01
 test time : 75.8 Train 0.0 Validation 0.5294412970542908 Test 0.5084869861602783
Epoch 25 train time : 456.1 loss: 2.43e-01
 test time : 75.8 Train 0.0 Validation 0.5325340032577515 Test 0.5063777565956116
Epoch 26 train time : 457.4 loss: 2.42e-01
 test time : 75.8 Train 0.0 Validation 0.5225141644477844 Test 0.5036377906799316
Epoch 27 train time : 458.9 loss: 2.44e-01
 test time : 76.8 Train 0.0 Validation 0.5403338670730591 Test 0.5098938941955566
Epoch 28 train time : 455.2 loss: 2.43e-01
 test time : 75.8 Train 0.0 Validation 0.5325219035148621 Test 0.5145655870437622
Epoch 29 train time : 458.7 loss: 2.41e-01
 test time : 75.9 Train 0.0 Validation 0.5252379179000854 Test 0.5146129727363586
Epoch 30 train time : 456.8 loss: 2.42e-01
 test time : 75.8 Train 0.0 Validation 0.4849773943424225 Test 0.4763695299625397
Epoch 31 train time : 460.1 loss: 2.39e-01
 test time : 75.8 Train 0.0 Validation 0.5375086069107056 Test 0.5170556902885437
Epoch 32 train time : 456.4 loss: 2.42e-01
 test time : 75.7 Train 0.0 Validation 0.5406824350357056 Test 0.5144253969192505
Epoch 33 train time : 456.6 loss: 2.38e-01
 test time : 75.8 Train 0.0 Validation 0.5321768522262573 Test 0.5067659020423889
Epoch 34 train time : 459.6 loss: 2.38e-01
 test time : 75.8 Train 0.0 Validation 0.5103575587272644 Test 0.4902658462524414
Epoch 35 train time : 455.2 loss: 2.39e-01
 test time : 75.8 Train 0.0 Validation 0.535948634147644 Test 0.5151923894882202
Epoch 36 train time : 457.6 loss: 2.40e-01
 test time : 75.8 Train 0.0 Validation 0.5288390517234802 Test 0.5127213597297668
Epoch 37 train time : 457.5 loss: 2.38e-01
 test time : 75.6 Train 0.0 Validation 0.5288964509963989 Test 0.4979473650455475
Epoch 38 train time : 457.1 loss: 2.37e-01
 test time : 75.8 Train 0.0 Validation 0.5179041624069214 Test 0.512651801109314
Epoch 39 train time : 458.9 loss: 2.36e-01
 test time : 75.9 Train 0.0 Validation 0.5243762731552124 Test 0.5205690264701843
Epoch 40 train time : 456.6 loss: 2.36e-01
 test time : 76.6 Train 0.0 Validation 0.5293287038803101 Test 0.5069714784622192
Epoch 41 train time : 457.4 loss: 2.35e-01
 test time : 75.8 Train 0.0 Validation 0.5313165783882141 Test 0.5186943411827087
Epoch 42 train time : 458.5 loss: 2.34e-01
 test time : 75.8 Train 0.0 Validation 0.532974898815155 Test 0.513576328754425
Epoch 43 train time : 457.9 loss: 2.34e-01
 test time : 75.4 Train 0.0 Validation 0.4111168384552002 Test 0.4164291024208069
Epoch 44 train time : 459.1 loss: 2.34e-01
 test time : 75.5 Train 0.0 Validation 0.5374342203140259 Test 0.5135617256164551
Epoch 45 train time : 457.3 loss: 2.34e-01
 test time : 75.4 Train 0.0 Validation 0.5117067694664001 Test 0.5086162686347961
Epoch 46 train time : 454.3 loss: 2.33e-01
 test time : 75.4 Train 0.0 Validation 0.527069091796875 Test 0.502266526222229
Epoch 47 train time : 454.7 loss: 2.32e-01
 test time : 75.5 Train 0.0 Validation 0.5262414216995239 Test 0.49626073241233826
Epoch 48 train time : 452.9 loss: 2.34e-01
 test time : 76.3 Train 0.0 Validation 0.5430806875228882 Test 0.5358953475952148
Epoch 49 train time : 454.4 loss: 2.33e-01
 test time : 75.4 Train 0.0 Validation 0.5608599781990051 Test 0.5270158052444458
Epoch 50 train time : 457.3 loss: 2.32e-01
 test time : 75.4 Train 0.0 Validation 0.5597034692764282 Test 0.538625180721283
Epoch 51 train time : 457.4 loss: 2.30e-01
 test time : 75.5 Train 0.0 Validation 0.5640819072723389 Test 0.5213343501091003
Epoch 52 train time : 453.7 loss: 2.30e-01
 test time : 75.4 Train 0.0 Validation 0.5319856405258179 Test 0.5146499872207642
Epoch 53 train time : 456.9 loss: 2.29e-01
 test time : 75.5 Train 0.0 Validation 0.5381728410720825 Test 0.5178443193435669
Epoch 54 train time : 456.2 loss: 2.27e-01
 test time : 75.4 Train 0.0 Validation 0.546444296836853 Test 0.5297739505767822
Epoch 55 train time : 455.2 loss: 2.26e-01
 test time : 75.4 Train 0.0 Validation 0.566277265548706 Test 0.5202401876449585
Epoch 56 train time : 458.4 loss: 2.23e-01
 test time : 75.4 Train 0.0 Validation 0.5501227974891663 Test 0.5265210270881653
Epoch 57 train time : 453.9 loss: 2.22e-01
 test time : 75.4 Train 0.0 Validation 0.5574763417243958 Test 0.5281980037689209
Epoch 58 train time : 454.9 loss: 2.20e-01
 test time : 75.5 Train 0.0 Validation 0.5625625848770142 Test 0.5295169949531555
Epoch 59 train time : 456.1 loss: 2.19e-01
 test time : 75.4 Train 0.0 Validation 0.5735985040664673 Test 0.5392192602157593
Epoch 60 train time : 457.9 loss: 2.14e-01
 test time : 75.5 Train 0.0 Validation 0.5782617926597595 Test 0.5681040287017822
Epoch 61 train time : 456.9 loss: 2.12e-01
 test time : 76.7 Train 0.0 Validation 0.5740981101989746 Test 0.5463442802429199
Epoch 62 train time : 459.4 loss: 2.08e-01
 test time : 75.5 Train 0.0 Validation 0.6041517853736877 Test 0.5724397897720337
Epoch 63 train time : 457.1 loss: 2.04e-01
 test time : 75.6 Train 0.0 Validation 0.5955162048339844 Test 0.5739831924438477
Epoch 64 train time : 456.2 loss: 2.00e-01
 test time : 75.4 Train 0.0 Validation 0.595889687538147 Test 0.5654021501541138
Epoch 65 train time : 458.9 loss: 1.95e-01
 test time : 75.5 Train 0.0 Validation 0.5891612768173218 Test 0.5644689798355103
Epoch 66 train time : 457.0 loss: 1.91e-01
 test time : 75.3 Train 0.0 Validation 0.6065765619277954 Test 0.6002514958381653
Epoch 67 train time : 454.7 loss: 1.87e-01
 test time : 75.4 Train 0.0 Validation 0.6111479997634888 Test 0.596873939037323
Epoch 68 train time : 454.0 loss: 1.81e-01
 test time : 75.4 Train 0.0 Validation 0.6156753897666931 Test 0.5940761566162109
Epoch 69 train time : 456.2 loss: 1.75e-01
 test time : 76.2 Train 0.0 Validation 0.6162090301513672 Test 0.6000361442565918
Epoch 70 train time : 454.8 loss: 1.69e-01
 test time : 75.7 Train 0.0 Validation 0.6185387372970581 Test 0.6057104468345642
Epoch 71 train time : 456.2 loss: 1.61e-01
 test time : 75.6 Train 0.0 Validation 0.6337255835533142 Test 0.6201153993606567
Epoch 72 train time : 458.0 loss: 1.53e-01
 test time : 75.8 Train 0.0 Validation 0.6282591819763184 Test 0.6191595792770386
Epoch 73 train time : 459.0 loss: 1.45e-01
 test time : 76.1 Train 0.0 Validation 0.6297494173049927 Test 0.6221638917922974
Epoch 74 train time : 456.6 loss: 1.38e-01
 test time : 75.9 Train 0.0 Validation 0.6346241235733032 Test 0.6214618682861328
Epoch 75 train time : 458.0 loss: 1.28e-01
 test time : 75.9 Train 0.0 Validation 0.629248321056366 Test 0.6176700592041016
Epoch 76 train time : 463.0 loss: 1.22e-01
 test time : 75.9 Train 0.0 Validation 0.6277063488960266 Test 0.6234278678894043
Epoch 77 train time : 456.7 loss: 1.15e-01
 test time : 76.1 Train 0.0 Validation 0.6308465600013733 Test 0.6260066032409668
Epoch 78 train time : 463.1 loss: 1.10e-01
 test time : 75.8 Train 0.0 Validation 0.6282275319099426 Test 0.6276016235351562
Epoch 79 train time : 459.7 loss: 1.06e-01
 test time : 76.1 Train 0.0 Validation 0.6265193223953247 Test 0.6266972422599792
Epoch 80 train time : 460.2 loss: 1.04e-01
 test time : 75.8 Train 0.0 Validation 0.6273674368858337 Test 0.6249189376831055
Best @73 validation score: 0.6346 Test score: 0.6215
[[73, tensor(0.6346), tensor(0.6215)]]
all runs:  73.0 0.6346241235733032 0.6214618682861328 0.0 0.0 0.0 
