/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
Namespace(dataset='pepstruct', repeat=1, num_workers=0, amp=True, compile=True, batch_size=6, testbatch_size=6, epochs=80, wd=0.1, lr=0.0001, beta=0.9, minlr=0.0, K=0.0, gradclipnorm=1.0, decompnoise=1e-06, seedoffset=0, warmstart=8, conststep=64, cosstep=8, use_y_scale=False, dp=0.0, eldp=0.0, act='silu', lossparam=0.0, advloss=False, embdp=0.0, embbn=False, emborthoinit=False, degreeemb=False, embln=True, featdim=-1, hiddim=88, caldim=-1, normA=False, laplacian=True, sqrtlambda=True, elres=True, usesvmix=True, vmean=True, vnorm=True, elvmean=True, elvnorm=True, snorm=True, gsizenorm=1.85, l_encoder='deepset', l_layers=3, l_combine='mul', l_aggr='mean', l_res=True, l_mlptailact1=True, l_mlplayers1=2, l_mlpnorm1='ln', l_mlptailact2=False, l_mlplayers2=0, l_mlpnorm2='none', num_layers=6, sv_uselinv=True, sv_tailact=True, sv_res=True, sv_numlayer=1, sv_norm='none', el_uselinv=True, el_uselins=False, el_tailact=True, el_numlayer=2, el_norm='none', el_uses=False, conv_uselinv=True, conv_tailact=True, conv_numlayer=1, conv_norm='none', predlin_numlayer=1, predlin_norm='none', lexp='mlp', lexp_layer=2, lexp_norm='ln', outln=False, pool='mean', Tm=1, save=None, load='pepstruct.wd1e-1', use_pos=False, align_size=32)
fixed l1reg
10873 2331 2331
split 10873 2331 2331
num_task 11
PiOModel(
  (inputencoder): QInputEncoder(
    (xemb): MultiEmbedding(
      (embedding_list): ModuleList(
        (0): Embedding(18, 88, padding_idx=0)
        (1): Embedding(4, 88, padding_idx=0)
        (2-3): 2 x Embedding(8, 88, padding_idx=0)
        (4): Embedding(6, 88, padding_idx=0)
        (5): Embedding(2, 88, padding_idx=0)
        (6): Embedding(7, 88, padding_idx=0)
        (7-8): 2 x Embedding(3, 88, padding_idx=0)
      )
      (postemb): Sequential(
        (0): LayerNorm((88,), eps=1e-05, elementwise_affine=False)
      )
    )
    (edgeEmb): MultiEmbedding(
      (embedding_list): ModuleList(
        (0-2): 3 x Embedding(5, 88, padding_idx=0)
      )
      (postemb): Sequential(
        (0): LayerNorm((88,), eps=1e-05, elementwise_affine=False)
      )
    )
    (LambdaEmb): MLPEncoding(
      (lin): MLP(
        (lin): Sequential(
          (0): Linear(in_features=1, out_features=176, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((176,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=176, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
  )
  (LambdaEncoder): PermEquiLayer(
    (set2set): Sequential(
      (0): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (1): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (2): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (3): Identity()
    )
    (set2vec): Sequential(
      (0): MLP(
      (lin): Sequential(
        (0): NoneNorm()
      )
    )
    )
  )
  (elprojs): ModuleList(
    (0-5): 6 x sv2el(
      (linv1): Linear(in_features=88, out_features=88, bias=False)
      (linv2): Linear(in_features=88, out_features=88, bias=False)
      (lins1): Identity()
      (lins2): Identity()
      (lin): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): NoneNorm()
          (5): SiLU(inplace=True)
        )
      )
    )
  )
  (svmixs): ModuleList(
    (0-5): 6 x svMix(
      (linv1): Linear(in_features=88, out_features=88, bias=False)
      (linv2): Linear(in_features=88, out_features=88, bias=False)
      (linv3): Identity()
      (lins1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (lins2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (lins3): Identity()
    )
  )
  (convs): ModuleList(
    (0-5): 6 x DirCFConv(
      (lins): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (linv): Linear(in_features=88, out_features=88, bias=False)
    )
  )
  (predlin): MLP(
    (lin): Sequential(
      (0): Linear(in_features=88, out_features=11, bias=True)
    )
  )
  (predln): Identity()
  (vln): Sequential(
    (0): VMean()
    (1): VNorm()
  )
  (elvln): Sequential(
    (0): VMean()
    (1): VNorm()
  )
  (sln): LayerNorm((88,), eps=1e-05, elementwise_affine=False)
)
numel 587323
mod/pepstruct.wd1e-1.0.pt
<All keys matched successfully>
None
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Epoch 1 train time : 1108.5 loss: 2.44e-01
 test time : 451.1 Train 0.0 Validation 0.2568405866622925 Test 0.26485714316368103
GPU memory 19.32
Epoch 2 train time : 418.1 loss: 2.47e-01
 test time : 65.9 Train 0.0 Validation 0.2557925283908844 Test 0.2623857259750366
Epoch 3 train time : 391.3 loss: 2.51e-01
 test time : 67.4 Train 0.0 Validation 0.25606614351272583 Test 0.26347121596336365
Epoch 4 train time : 403.0 loss: 2.53e-01
 test time : 76.5 Train 0.0 Validation 0.25907471776008606 Test 0.26682159304618835
Epoch 5 train time : 421.1 loss: 2.59e-01
 test time : 76.1 Train 0.0 Validation 0.2624037563800812 Test 0.27059856057167053
Epoch 6 train time : 472.6 loss: 2.62e-01
 test time : 67.7 Train 0.0 Validation 0.2643851041793823 Test 0.2707631289958954
Epoch 7 train time : 402.5 loss: 2.64e-01
 test time : 75.6 Train 0.0 Validation 0.2622103691101074 Test 0.2693013548851013
Epoch 8 train time : 423.6 loss: 2.69e-01
 test time : 74.6 Train 0.0 Validation 0.27375170588493347 Test 0.28256142139434814
Epoch 9 train time : 393.7 loss: 2.70e-01
 test time : 66.5 Train 0.0 Validation 0.27192795276641846 Test 0.2752276659011841
Epoch 10 train time : 382.4 loss: 2.71e-01
 test time : 69.0 Train 0.0 Validation 0.26951169967651367 Test 0.27319520711898804
Epoch 11 train time : 404.3 loss: 2.70e-01
 test time : 70.9 Train 0.0 Validation 0.27138498425483704 Test 0.2789361774921417
Epoch 12 train time : 394.3 loss: 2.70e-01
 test time : 65.4 Train 0.0 Validation 0.26348623633384705 Test 0.26943138241767883
Epoch 13 train time : 383.7 loss: 2.72e-01
 test time : 65.8 Train 0.0 Validation 0.2616538405418396 Test 0.2683248519897461
Epoch 14 train time : 393.1 loss: 2.71e-01
 test time : 70.6 Train 0.0 Validation 0.27404680848121643 Test 0.2789155840873718
Epoch 15 train time : 403.0 loss: 2.69e-01
 test time : 71.8 Train 0.0 Validation 0.2661972939968109 Test 0.27513307332992554
Epoch 16 train time : 382.8 loss: 2.68e-01
 test time : 65.7 Train 0.0 Validation 0.26503515243530273 Test 0.2687746286392212
Epoch 17 train time : 384.6 loss: 2.70e-01
 test time : 71.6 Train 0.0 Validation 0.2670588195323944 Test 0.2684333920478821
Epoch 18 train time : 403.0 loss: 2.68e-01
 test time : 70.3 Train 0.0 Validation 0.27045679092407227 Test 0.27630704641342163
Epoch 19 train time : 389.0 loss: 2.69e-01
 test time : 63.2 Train 0.0 Validation 0.27641525864601135 Test 0.2840240001678467
Epoch 20 train time : 364.2 loss: 2.69e-01
 test time : 63.1 Train 0.0 Validation 0.29910725355148315 Test 0.30888140201568604
Epoch 21 train time : 364.9 loss: 2.70e-01
 test time : 63.0 Train 0.0 Validation 0.26515185832977295 Test 0.2716638445854187
Epoch 22 train time : 366.3 loss: 2.70e-01
 test time : 63.2 Train 0.0 Validation 0.2664692997932434 Test 0.2746092677116394
Epoch 23 train time : 365.2 loss: 2.69e-01
 test time : 63.2 Train 0.0 Validation 0.27326181530952454 Test 0.2804715037345886
Epoch 24 train time : 364.7 loss: 2.66e-01
 test time : 63.1 Train 0.0 Validation 0.2723647356033325 Test 0.2761220633983612
Epoch 25 train time : 366.7 loss: 2.68e-01
 test time : 63.1 Train 0.0 Validation 0.27078038454055786 Test 0.279794842004776
Epoch 26 train time : 365.2 loss: 2.69e-01
 test time : 63.1 Train 0.0 Validation 0.26073145866394043 Test 0.2714748680591583
Epoch 27 train time : 366.0 loss: 2.68e-01
 test time : 63.1 Train 0.0 Validation 0.26720163226127625 Test 0.273985356092453
Epoch 28 train time : 369.6 loss: 2.68e-01
 test time : 63.7 Train 0.0 Validation 0.2674686908721924 Test 0.2757221460342407
Epoch 29 train time : 370.6 loss: 2.69e-01
 test time : 63.7 Train 0.0 Validation 0.2687847316265106 Test 0.2726268470287323
Epoch 30 train time : 369.2 loss: 2.68e-01
 test time : 63.6 Train 0.0 Validation 0.26935264468193054 Test 0.272857129573822
Epoch 31 train time : 370.6 loss: 2.67e-01
 test time : 63.9 Train 0.0 Validation 0.2783966362476349 Test 0.28547897934913635
Epoch 32 train time : 369.8 loss: 2.68e-01
 test time : 63.6 Train 0.0 Validation 0.2592802941799164 Test 0.2679932713508606
Epoch 33 train time : 371.1 loss: 2.66e-01
 test time : 63.9 Train 0.0 Validation 0.2658998668193817 Test 0.274272620677948
Epoch 34 train time : 372.4 loss: 2.67e-01
 test time : 63.7 Train 0.0 Validation 0.27167680859565735 Test 0.2763877809047699
Epoch 35 train time : 370.6 loss: 2.66e-01
 test time : 63.9 Train 0.0 Validation 0.29911160469055176 Test 0.3039136230945587
Epoch 36 train time : 372.1 loss: 2.66e-01
 test time : 64.0 Train 0.0 Validation 0.25959980487823486 Test 0.2669258117675781
Epoch 37 train time : 370.0 loss: 2.67e-01
 test time : 63.9 Train 0.0 Validation 0.2599707841873169 Test 0.26611942052841187
Epoch 38 train time : 373.4 loss: 2.65e-01
 test time : 63.8 Train 0.0 Validation 0.25958895683288574 Test 0.2660549581050873
Epoch 39 train time : 370.9 loss: 2.65e-01
 test time : 63.9 Train 0.0 Validation 0.2701983153820038 Test 0.2767804265022278
Epoch 40 train time : 371.6 loss: 2.65e-01
 test time : 63.9 Train 0.0 Validation 0.28339672088623047 Test 0.29953017830848694
Epoch 41 train time : 370.4 loss: 2.66e-01
 test time : 63.8 Train 0.0 Validation 0.27296748757362366 Test 0.2802913188934326
Epoch 42 train time : 372.1 loss: 2.65e-01
 test time : 63.9 Train 0.0 Validation 0.262177050113678 Test 0.26405665278434753
Epoch 43 train time : 371.7 loss: 2.64e-01
 test time : 63.8 Train 0.0 Validation 0.263605535030365 Test 0.27144068479537964
Epoch 44 train time : 368.2 loss: 2.65e-01
 test time : 63.8 Train 0.0 Validation 0.27007806301116943 Test 0.28074976801872253
Epoch 45 train time : 370.8 loss: 2.63e-01
 test time : 63.8 Train 0.0 Validation 0.2626569867134094 Test 0.2671505808830261
Epoch 46 train time : 371.9 loss: 2.64e-01
 test time : 63.8 Train 0.0 Validation 0.2637825906276703 Test 0.27036336064338684
Epoch 47 train time : 374.2 loss: 2.65e-01
 test time : 63.8 Train 0.0 Validation 0.2697044014930725 Test 0.27525949478149414
Epoch 48 train time : 369.1 loss: 2.66e-01
 test time : 63.9 Train 0.0 Validation 0.26174604892730713 Test 0.26704171299934387
Epoch 49 train time : 370.8 loss: 2.64e-01
 test time : 63.8 Train 0.0 Validation 0.26788365840911865 Test 0.2712048888206482
Epoch 50 train time : 369.6 loss: 2.64e-01
 test time : 63.7 Train 0.0 Validation 0.2585126757621765 Test 0.26475316286087036
Epoch 51 train time : 371.8 loss: 2.66e-01
 test time : 63.9 Train 0.0 Validation 0.2636564373970032 Test 0.27074241638183594
Epoch 52 train time : 369.7 loss: 2.66e-01
 test time : 63.8 Train 0.0 Validation 0.2889775037765503 Test 0.2935344874858856
Epoch 53 train time : 371.2 loss: 2.64e-01
 test time : 63.9 Train 0.0 Validation 0.28104904294013977 Test 0.2918933629989624
Epoch 54 train time : 373.2 loss: 2.64e-01
 test time : 63.8 Train 0.0 Validation 0.2631165087223053 Test 0.2710830271244049
Epoch 55 train time : 370.9 loss: 2.65e-01
 test time : 63.9 Train 0.0 Validation 0.2586580514907837 Test 0.2629256546497345
Epoch 56 train time : 369.2 loss: 2.61e-01
 test time : 63.8 Train 0.0 Validation 0.26118990778923035 Test 0.2695423364639282
Epoch 57 train time : 370.5 loss: 2.63e-01
 test time : 63.9 Train 0.0 Validation 0.2755963206291199 Test 0.28740039467811584
Epoch 58 train time : 374.1 loss: 2.65e-01
 test time : 63.8 Train 0.0 Validation 0.277028888463974 Test 0.2922077178955078
Epoch 59 train time : 371.6 loss: 2.64e-01
 test time : 63.8 Train 0.0 Validation 0.261505663394928 Test 0.26651158928871155
Epoch 60 train time : 373.5 loss: 2.63e-01
 test time : 63.8 Train 0.0 Validation 0.27666977047920227 Test 0.2821130156517029
Epoch 61 train time : 370.0 loss: 2.64e-01
 test time : 63.8 Train 0.0 Validation 0.26147735118865967 Test 0.2651880979537964
Epoch 62 train time : 367.9 loss: 2.63e-01
 test time : 63.8 Train 0.0 Validation 0.2632809281349182 Test 0.2721767723560333
Epoch 63 train time : 369.8 loss: 2.64e-01
 test time : 63.7 Train 0.0 Validation 0.2618553936481476 Test 0.26664435863494873
Epoch 64 train time : 370.5 loss: 2.63e-01
 test time : 63.9 Train 0.0 Validation 0.27325838804244995 Test 0.28000304102897644
Epoch 65 train time : 367.5 loss: 2.62e-01
 test time : 63.7 Train 0.0 Validation 0.28388407826423645 Test 0.2918829619884491
Epoch 66 train time : 372.7 loss: 2.63e-01
 test time : 63.9 Train 0.0 Validation 0.2602168023586273 Test 0.26977506279945374
Epoch 67 train time : 369.8 loss: 2.62e-01
 test time : 63.7 Train 0.0 Validation 0.2606629729270935 Test 0.2697041928768158
Epoch 68 train time : 370.6 loss: 2.62e-01
 test time : 63.8 Train 0.0 Validation 0.2863512337207794 Test 0.2872864305973053
Epoch 69 train time : 372.1 loss: 2.61e-01
 test time : 63.8 Train 0.0 Validation 0.263249009847641 Test 0.2687394618988037
Epoch 70 train time : 370.0 loss: 2.61e-01
 test time : 63.9 Train 0.0 Validation 0.2603415846824646 Test 0.26993754506111145
Epoch 71 train time : 370.2 loss: 2.62e-01
 test time : 63.7 Train 0.0 Validation 0.26147496700286865 Test 0.27002283930778503
Epoch 72 train time : 370.6 loss: 2.61e-01
 test time : 63.9 Train 0.0 Validation 0.26876917481422424 Test 0.2816794514656067
Epoch 73 train time : 370.4 loss: 2.59e-01
 test time : 63.8 Train 0.0 Validation 0.2709537744522095 Test 0.27904966473579407
Epoch 74 train time : 367.4 loss: 2.59e-01
 test time : 63.9 Train 0.0 Validation 0.26641845703125 Test 0.2764419615268707
Epoch 75 train time : 368.9 loss: 2.55e-01
 test time : 63.8 Train 0.0 Validation 0.2764352262020111 Test 0.28154459595680237
Epoch 76 train time : 372.1 loss: 2.50e-01
 test time : 63.8 Train 0.0 Validation 0.2545382082462311 Test 0.2603134214878082
Epoch 77 train time : 368.8 loss: 2.44e-01
 test time : 63.8 Train 0.0 Validation 0.25851088762283325 Test 0.2640743851661682
Epoch 78 train time : 372.1 loss: 2.37e-01
 test time : 63.7 Train 0.0 Validation 0.2533315420150757 Test 0.263415664434433
Epoch 79 train time : 368.7 loss: 2.31e-01
 test time : 63.8 Train 0.0 Validation 0.25095221400260925 Test 0.2592446804046631
Epoch 80 train time : 368.6 loss: 2.29e-01
 test time : 63.6 Train 0.0 Validation 0.24914146959781647 Test 0.25761643052101135
Best @79 validation score: 0.2491 Test score: 0.2576
[[79, tensor(0.2491), tensor(0.2576)]]
all runs:  79.0 0.24914146959781647 0.25761643052101135 0.0 0.0 0.0 
