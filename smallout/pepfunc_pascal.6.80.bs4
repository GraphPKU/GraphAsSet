/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
Namespace(dataset='pepfunc', repeat=1, num_workers=0, amp=True, compile=True, batch_size=4, testbatch_size=4, epochs=80, wd=0.1, lr=0.001, beta=0.9, minlr=0.0, K=0.0, gradclipnorm=1.0, decompnoise=1e-06, seedoffset=0, warmstart=8, conststep=40, cosstep=32, use_y_scale=False, dp=0.0, eldp=0.0, act='silu', lossparam=0.0, advloss=False, embdp=0.0, embbn=False, emborthoinit=False, degreeemb=False, embln=True, featdim=-1, hiddim=80, caldim=-1, normA=False, laplacian=True, sqrtlambda=True, elres=True, usesvmix=True, vmean=True, vnorm=True, elvmean=True, elvnorm=True, snorm=True, gsizenorm=1.85, l_encoder='deepset', l_layers=3, l_combine='mul', l_aggr='mean', l_res=True, l_mlptailact1=True, l_mlplayers1=2, l_mlpnorm1='ln', l_mlptailact2=False, l_mlplayers2=0, l_mlpnorm2='none', num_layers=6, sv_uselinv=True, sv_tailact=True, sv_res=True, sv_numlayer=1, sv_norm='none', el_uselinv=True, el_uselins=False, el_tailact=True, el_numlayer=2, el_norm='none', el_uses=False, conv_uselinv=True, conv_tailact=True, conv_numlayer=1, conv_norm='none', predlin_numlayer=1, predlin_norm='none', lexp='mlp', lexp_layer=2, lexp_norm='ln', outln=False, pool='mean', Tm=1, save=None, load=None, use_pos=False, align_size=32)
fixed bincls
10873 2331 2331
split 10873 2331 2331
num_task 10
PiOModel(
  (inputencoder): QInputEncoder(
    (xemb): MultiEmbedding(
      (embedding_list): ModuleList(
        (0): Embedding(18, 80, padding_idx=0)
        (1): Embedding(4, 80, padding_idx=0)
        (2-3): 2 x Embedding(8, 80, padding_idx=0)
        (4): Embedding(6, 80, padding_idx=0)
        (5): Embedding(2, 80, padding_idx=0)
        (6): Embedding(7, 80, padding_idx=0)
        (7-8): 2 x Embedding(3, 80, padding_idx=0)
      )
      (postemb): Sequential(
        (0): LayerNorm((80,), eps=1e-05, elementwise_affine=False)
      )
    )
    (edgeEmb): MultiEmbedding(
      (embedding_list): ModuleList(
        (0-2): 3 x Embedding(5, 80, padding_idx=0)
      )
      (postemb): Sequential(
        (0): LayerNorm((80,), eps=1e-05, elementwise_affine=False)
      )
    )
    (LambdaEmb): MLPEncoding(
      (lin): MLP(
        (lin): Sequential(
          (0): Linear(in_features=1, out_features=160, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=160, out_features=80, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
    (distEmb): MLPEncoding(
      (lin): MLP(
        (lin): Sequential(
          (0): Linear(in_features=1, out_features=160, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=160, out_features=80, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
  )
  (LambdaEncoder): PermEquiLayer(
    (set2set): Sequential(
      (0): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=80, out_features=80, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=80, out_features=80, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=80, out_features=80, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=80, out_features=80, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (1): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=80, out_features=80, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=80, out_features=80, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=80, out_features=80, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=80, out_features=80, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (2): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=80, out_features=80, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=80, out_features=80, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=80, out_features=80, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=80, out_features=80, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (3): Identity()
    )
    (set2vec): Sequential(
      (0): MLP(
      (lin): Sequential(
        (0): NoneNorm()
      )
    )
    )
  )
  (elprojs): ModuleList(
    (0-5): 6 x sv2el(
      (linv1): Linear(in_features=80, out_features=80, bias=False)
      (linv2): Linear(in_features=80, out_features=80, bias=False)
      (lins1): Identity()
      (lins2): Identity()
      (lin): MLP(
        (lin): Sequential(
          (0): Linear(in_features=80, out_features=80, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
          (3): Linear(in_features=80, out_features=80, bias=True)
          (4): NoneNorm()
          (5): SiLU(inplace=True)
        )
      )
    )
  )
  (svmixs): ModuleList(
    (0-5): 6 x svMix(
      (linv1): Linear(in_features=80, out_features=80, bias=False)
      (linv2): Linear(in_features=80, out_features=80, bias=False)
      (linv3): Identity()
      (lins1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=80, out_features=80, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (lins2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=80, out_features=80, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (lins3): Identity()
    )
  )
  (convs): ModuleList(
    (0-5): 6 x DirCFConv(
      (lins): MLP(
        (lin): Sequential(
          (0): Linear(in_features=80, out_features=80, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (linv): Linear(in_features=80, out_features=80, bias=False)
    )
  )
  (predlin): MLP(
    (lin): Sequential(
      (0): Linear(in_features=80, out_features=10, bias=True)
    )
  )
  (predln): Identity()
  (vln): Sequential(
    (0): VMean()
    (1): VNorm()
  )
  (elvln): Sequential(
    (0): VMean()
    (1): VNorm()
  )
  (sln): LayerNorm((80,), eps=1e-05, elementwise_affine=False)
)
numel 500170
Epoch 1 train time : 1026.3 loss: 3.90e-01
 test time : 440.3 Train 0.0 Validation 0.30293866991996765 Test 0.30099791288375854
GPU memory 11.82
Epoch 2 train time : 305.9 loss: 3.17e-01
 test time : 52.2 Train 0.0 Validation 0.37200456857681274 Test 0.37140926718711853
Epoch 3 train time : 307.8 loss: 2.96e-01
 test time : 52.6 Train 0.0 Validation 0.41030535101890564 Test 0.40252041816711426
Epoch 4 train time : 307.0 loss: 2.89e-01
 test time : 52.2 Train 0.0 Validation 0.4152919352054596 Test 0.4015616476535797
Epoch 5 train time : 306.6 loss: 2.82e-01
 test time : 52.3 Train 0.0 Validation 0.43853840231895447 Test 0.41213154792785645
Epoch 6 train time : 307.7 loss: 2.78e-01
 test time : 52.5 Train 0.0 Validation 0.44017845392227173 Test 0.4236915707588196
Epoch 7 train time : 309.8 loss: 2.73e-01
 test time : 52.3 Train 0.0 Validation 0.46532803773880005 Test 0.4336656928062439
Epoch 8 train time : 306.3 loss: 2.70e-01
 test time : 52.5 Train 0.0 Validation 0.472592830657959 Test 0.4470810890197754
Epoch 9 train time : 307.0 loss: 2.67e-01
 test time : 52.3 Train 0.0 Validation 0.4696292281150818 Test 0.45335888862609863
Epoch 10 train time : 308.3 loss: 2.64e-01
 test time : 52.2 Train 0.0 Validation 0.47855788469314575 Test 0.45469599962234497
Epoch 11 train time : 309.1 loss: 2.60e-01
 test time : 52.6 Train 0.0 Validation 0.4673129916191101 Test 0.4538814127445221
Epoch 12 train time : 308.6 loss: 2.58e-01
 test time : 52.3 Train 0.0 Validation 0.43815645575523376 Test 0.4213857054710388
Epoch 13 train time : 306.5 loss: 2.56e-01
 test time : 52.5 Train 0.0 Validation 0.4539148211479187 Test 0.42988887429237366
Epoch 14 train time : 309.3 loss: 2.53e-01
 test time : 52.7 Train 0.0 Validation 0.4793960452079773 Test 0.44997137784957886
Epoch 15 train time : 307.9 loss: 2.53e-01
 test time : 52.3 Train 0.0 Validation 0.5182400941848755 Test 0.4875599443912506
Epoch 16 train time : 307.6 loss: 2.51e-01
 test time : 52.6 Train 0.0 Validation 0.5242656469345093 Test 0.49005693197250366
Epoch 17 train time : 307.7 loss: 2.49e-01
 test time : 52.4 Train 0.0 Validation 0.5122789144515991 Test 0.48986077308654785
Epoch 18 train time : 306.8 loss: 2.49e-01
 test time : 52.5 Train 0.0 Validation 0.500083327293396 Test 0.49005040526390076
Epoch 19 train time : 308.8 loss: 2.48e-01
 test time : 52.4 Train 0.0 Validation 0.5276914834976196 Test 0.49630969762802124
Epoch 20 train time : 305.6 loss: 2.47e-01
 test time : 52.9 Train 0.0 Validation 0.5077956914901733 Test 0.4817545413970947
Epoch 21 train time : 309.1 loss: 2.45e-01
 test time : 52.6 Train 0.0 Validation 0.5080732107162476 Test 0.4851156771183014
Epoch 22 train time : 308.8 loss: 2.46e-01
 test time : 52.2 Train 0.0 Validation 0.5113470554351807 Test 0.4896883964538574
Epoch 23 train time : 307.2 loss: 2.44e-01
 test time : 52.2 Train 0.0 Validation 0.4946970045566559 Test 0.48860111832618713
Epoch 24 train time : 308.2 loss: 2.44e-01
 test time : 52.5 Train 0.0 Validation 0.5433668494224548 Test 0.5110982656478882
Epoch 25 train time : 307.0 loss: 2.42e-01
 test time : 52.4 Train 0.0 Validation 0.5243717432022095 Test 0.5123204588890076
Epoch 26 train time : 307.6 loss: 2.41e-01
 test time : 52.4 Train 0.0 Validation 0.5314651727676392 Test 0.5225886702537537
Epoch 27 train time : 308.1 loss: 2.41e-01
 test time : 53.0 Train 0.0 Validation 0.5327778458595276 Test 0.4980061948299408
Epoch 28 train time : 307.1 loss: 2.41e-01
 test time : 52.2 Train 0.0 Validation 0.550991952419281 Test 0.5171903371810913
Epoch 29 train time : 307.8 loss: 2.40e-01
 test time : 52.5 Train 0.0 Validation 0.5460376739501953 Test 0.5304020047187805
Epoch 30 train time : 308.0 loss: 2.39e-01
 test time : 52.4 Train 0.0 Validation 0.5377333164215088 Test 0.5166622400283813
Epoch 31 train time : 306.0 loss: 2.40e-01
 test time : 52.4 Train 0.0 Validation 0.5260351896286011 Test 0.5036095380783081
Epoch 32 train time : 309.1 loss: 2.38e-01
 test time : 52.5 Train 0.0 Validation 0.5355201959609985 Test 0.506755530834198
Epoch 33 train time : 305.0 loss: 2.38e-01
 test time : 52.4 Train 0.0 Validation 0.5103740096092224 Test 0.4973176419734955
Epoch 34 train time : 306.7 loss: 2.36e-01
 test time : 53.1 Train 0.0 Validation 0.5132416486740112 Test 0.5005365610122681
Epoch 35 train time : 308.0 loss: 2.36e-01
 test time : 52.4 Train 0.0 Validation 0.5160596966743469 Test 0.48206591606140137
Epoch 36 train time : 309.0 loss: 2.37e-01
 test time : 52.4 Train 0.0 Validation 0.5563319325447083 Test 0.5310361981391907
Epoch 37 train time : 306.9 loss: 2.36e-01
 test time : 52.5 Train 0.0 Validation 0.5062988996505737 Test 0.48443490266799927
Epoch 38 train time : 307.4 loss: 2.37e-01
 test time : 52.4 Train 0.0 Validation 0.5347093939781189 Test 0.5074605345726013
Epoch 39 train time : 307.9 loss: 2.35e-01
 test time : 52.4 Train 0.0 Validation 0.4989064335823059 Test 0.4761260449886322
Epoch 40 train time : 308.8 loss: 2.36e-01
 test time : 52.4 Train 0.0 Validation 0.5393803119659424 Test 0.5121104121208191
Epoch 41 train time : 307.1 loss: 2.36e-01
 test time : 52.2 Train 0.0 Validation 0.5339192748069763 Test 0.5190443992614746
Epoch 42 train time : 308.8 loss: 2.35e-01
 test time : 52.5 Train 0.0 Validation 0.5159528255462646 Test 0.4820122718811035
Epoch 43 train time : 325.8 loss: 2.35e-01
 test time : 52.2 Train 0.0 Validation 0.5557073354721069 Test 0.5211457014083862
Epoch 44 train time : 305.4 loss: 2.34e-01
 test time : 52.4 Train 0.0 Validation 0.5441350340843201 Test 0.5131415128707886
Epoch 45 train time : 307.7 loss: 2.34e-01
 test time : 52.4 Train 0.0 Validation 0.553231418132782 Test 0.5151318907737732
Epoch 46 train time : 307.4 loss: 2.34e-01
 test time : 52.4 Train 0.0 Validation 0.5362300872802734 Test 0.5078799724578857
Epoch 47 train time : 308.3 loss: 2.34e-01
 test time : 52.6 Train 0.0 Validation 0.5512422323226929 Test 0.5138546228408813
Epoch 48 train time : 306.4 loss: 2.32e-01
 test time : 53.0 Train 0.0 Validation 0.53863525390625 Test 0.5053128004074097
Epoch 49 train time : 306.8 loss: 2.34e-01
 test time : 52.3 Train 0.0 Validation 0.4998549818992615 Test 0.48948726058006287
Epoch 50 train time : 306.9 loss: 2.31e-01
 test time : 52.5 Train 0.0 Validation 0.5370237827301025 Test 0.512100100517273
Epoch 51 train time : 307.4 loss: 2.32e-01
 test time : 52.4 Train 0.0 Validation 0.5463981032371521 Test 0.5012516379356384
Epoch 52 train time : 308.4 loss: 2.30e-01
 test time : 52.6 Train 0.0 Validation 0.5440142750740051 Test 0.5237439274787903
Epoch 53 train time : 309.0 loss: 2.29e-01
 test time : 52.3 Train 0.0 Validation 0.5695039629936218 Test 0.5313007235527039
Epoch 54 train time : 308.3 loss: 2.27e-01
 test time : 52.3 Train 0.0 Validation 0.559083104133606 Test 0.5268760323524475
Epoch 55 train time : 309.2 loss: 2.27e-01
 test time : 52.4 Train 0.0 Validation 0.5899674296379089 Test 0.5582226514816284
Epoch 56 train time : 306.8 loss: 2.24e-01
 test time : 52.3 Train 0.0 Validation 0.5634503364562988 Test 0.5303181409835815
Epoch 57 train time : 308.1 loss: 2.22e-01
 test time : 52.5 Train 0.0 Validation 0.5801758766174316 Test 0.5478760004043579
Epoch 58 train time : 307.9 loss: 2.19e-01
 test time : 52.4 Train 0.0 Validation 0.5834708213806152 Test 0.543606698513031
Epoch 59 train time : 308.8 loss: 2.20e-01
 test time : 52.3 Train 0.0 Validation 0.5628688335418701 Test 0.53285151720047
Epoch 60 train time : 307.7 loss: 2.16e-01
 test time : 52.5 Train 0.0 Validation 0.5441347360610962 Test 0.5129672288894653
Epoch 61 train time : 309.5 loss: 2.12e-01
 test time : 52.4 Train 0.0 Validation 0.5636838674545288 Test 0.5413759350776672
Epoch 62 train time : 306.9 loss: 2.10e-01
 test time : 52.5 Train 0.0 Validation 0.6029363870620728 Test 0.554358184337616
Epoch 63 train time : 308.4 loss: 2.05e-01
 test time : 52.4 Train 0.0 Validation 0.5941287279129028 Test 0.5705277323722839
Epoch 64 train time : 309.3 loss: 2.03e-01
 test time : 52.2 Train 0.0 Validation 0.6058579087257385 Test 0.5758605599403381
Epoch 65 train time : 306.7 loss: 1.97e-01
 test time : 52.5 Train 0.0 Validation 0.6141210794448853 Test 0.5831225514411926
Epoch 66 train time : 307.3 loss: 1.93e-01
 test time : 52.3 Train 0.0 Validation 0.6037060022354126 Test 0.5665438175201416
Epoch 67 train time : 307.9 loss: 1.88e-01
 test time : 52.6 Train 0.0 Validation 0.6071081161499023 Test 0.5764102339744568
Epoch 68 train time : 306.1 loss: 1.81e-01
 test time : 52.4 Train 0.0 Validation 0.608199954032898 Test 0.5779358744621277
Epoch 69 train time : 306.2 loss: 1.79e-01
 test time : 52.4 Train 0.0 Validation 0.6279727220535278 Test 0.5959980487823486
Epoch 70 train time : 309.0 loss: 1.72e-01
 test time : 52.5 Train 0.0 Validation 0.628086268901825 Test 0.599260151386261
Epoch 71 train time : 307.1 loss: 1.64e-01
 test time : 53.0 Train 0.0 Validation 0.6316308379173279 Test 0.6026450395584106
Epoch 72 train time : 307.3 loss: 1.57e-01
 test time : 52.5 Train 0.0 Validation 0.6437127590179443 Test 0.6077691316604614
Epoch 73 train time : 307.5 loss: 1.50e-01
 test time : 52.4 Train 0.0 Validation 0.6364654302597046 Test 0.6086386442184448
Epoch 74 train time : 307.3 loss: 1.44e-01
 test time : 52.4 Train 0.0 Validation 0.6396985650062561 Test 0.616660475730896
Epoch 75 train time : 306.5 loss: 1.36e-01
 test time : 52.7 Train 0.0 Validation 0.6423813700675964 Test 0.6252263784408569
Epoch 76 train time : 305.2 loss: 1.30e-01
 test time : 52.4 Train 0.0 Validation 0.6452574729919434 Test 0.6257899403572083
Epoch 77 train time : 309.1 loss: 1.25e-01
 test time : 53.6 Train 0.0 Validation 0.6517550349235535 Test 0.6214529275894165
Epoch 78 train time : 307.7 loss: 1.20e-01
 test time : 52.5 Train 0.0 Validation 0.6476300954818726 Test 0.6235755681991577
Epoch 79 train time : 308.6 loss: 1.17e-01
 test time : 52.5 Train 0.0 Validation 0.648078978061676 Test 0.6254398822784424
Epoch 80 train time : 307.7 loss: 1.15e-01
 test time : 52.7 Train 0.0 Validation 0.6460379362106323 Test 0.6248323321342468
Best @76 validation score: 0.6518 Test score: 0.6215
[[76, tensor(0.6518), tensor(0.6215)]]
all runs:  76.0 0.6517550349235535 0.6214529275894165 0.0 0.0 0.0 
