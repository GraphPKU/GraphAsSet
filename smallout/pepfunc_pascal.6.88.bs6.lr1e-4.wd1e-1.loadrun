/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
Namespace(dataset='pepfunc', repeat=1, num_workers=0, amp=True, compile=True, batch_size=6, testbatch_size=6, epochs=80, wd=0.1, lr=0.0001, beta=0.9, minlr=0.0, K=0.0, gradclipnorm=1.0, decompnoise=1e-06, seedoffset=0, warmstart=8, conststep=64, cosstep=8, use_y_scale=False, dp=0.0, eldp=0.0, act='silu', lossparam=0.0, advloss=False, embdp=0.0, embbn=False, emborthoinit=False, degreeemb=False, embln=True, featdim=-1, hiddim=88, caldim=-1, normA=False, laplacian=True, sqrtlambda=True, elres=True, usesvmix=True, vmean=True, vnorm=True, elvmean=True, elvnorm=True, snorm=True, gsizenorm=1.85, l_encoder='deepset', l_layers=3, l_combine='mul', l_aggr='mean', l_res=True, l_mlptailact1=True, l_mlplayers1=2, l_mlpnorm1='ln', l_mlptailact2=False, l_mlplayers2=0, l_mlpnorm2='none', num_layers=6, sv_uselinv=True, sv_tailact=True, sv_res=True, sv_numlayer=1, sv_norm='none', el_uselinv=True, el_uselins=False, el_tailact=True, el_numlayer=2, el_norm='none', el_uses=False, conv_uselinv=True, conv_tailact=True, conv_numlayer=1, conv_norm='none', predlin_numlayer=1, predlin_norm='none', lexp='mlp', lexp_layer=2, lexp_norm='ln', outln=False, pool='mean', Tm=1, save=None, load='pepfunc.wd1e-1', use_pos=False, align_size=32)
fixed bincls
10873 2331 2331
split 10873 2331 2331
num_task 10
PiOModel(
  (inputencoder): QInputEncoder(
    (xemb): MultiEmbedding(
      (embedding_list): ModuleList(
        (0): Embedding(18, 88, padding_idx=0)
        (1): Embedding(4, 88, padding_idx=0)
        (2-3): 2 x Embedding(8, 88, padding_idx=0)
        (4): Embedding(6, 88, padding_idx=0)
        (5): Embedding(2, 88, padding_idx=0)
        (6): Embedding(7, 88, padding_idx=0)
        (7-8): 2 x Embedding(3, 88, padding_idx=0)
      )
      (postemb): Sequential(
        (0): LayerNorm((88,), eps=1e-05, elementwise_affine=False)
      )
    )
    (edgeEmb): MultiEmbedding(
      (embedding_list): ModuleList(
        (0-2): 3 x Embedding(5, 88, padding_idx=0)
      )
      (postemb): Sequential(
        (0): LayerNorm((88,), eps=1e-05, elementwise_affine=False)
      )
    )
    (LambdaEmb): MLPEncoding(
      (lin): MLP(
        (lin): Sequential(
          (0): Linear(in_features=1, out_features=176, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((176,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=176, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
  )
  (LambdaEncoder): PermEquiLayer(
    (set2set): Sequential(
      (0): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (1): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (2): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (3): Identity()
    )
    (set2vec): Sequential(
      (0): MLP(
      (lin): Sequential(
        (0): NoneNorm()
      )
    )
    )
  )
  (elprojs): ModuleList(
    (0-5): 6 x sv2el(
      (linv1): Linear(in_features=88, out_features=88, bias=False)
      (linv2): Linear(in_features=88, out_features=88, bias=False)
      (lins1): Identity()
      (lins2): Identity()
      (lin): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): NoneNorm()
          (5): SiLU(inplace=True)
        )
      )
    )
  )
  (svmixs): ModuleList(
    (0-5): 6 x svMix(
      (linv1): Linear(in_features=88, out_features=88, bias=False)
      (linv2): Linear(in_features=88, out_features=88, bias=False)
      (linv3): Identity()
      (lins1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (lins2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (lins3): Identity()
    )
  )
  (convs): ModuleList(
    (0-5): 6 x DirCFConv(
      (lins): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (linv): Linear(in_features=88, out_features=88, bias=False)
    )
  )
  (predlin): MLP(
    (lin): Sequential(
      (0): Linear(in_features=88, out_features=10, bias=True)
    )
  )
  (predln): Identity()
  (vln): Sequential(
    (0): VMean()
    (1): VNorm()
  )
  (elvln): Sequential(
    (0): VMean()
    (1): VNorm()
  )
  (sln): LayerNorm((88,), eps=1e-05, elementwise_affine=False)
)
numel 587234
mod/pepfunc.wd1e-1.0.pt
<All keys matched successfully>
None
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Epoch 1 train time : 1035.9 loss: 1.06e-01
 test time : 569.5 Train 0.0 Validation 0.6364809274673462 Test 0.6310112476348877
GPU memory 19.28
Epoch 2 train time : 433.6 loss: 1.14e-01
 test time : 65.4 Train 0.0 Validation 0.6292068362236023 Test 0.6189072728157043
Epoch 3 train time : 380.4 loss: 1.20e-01
 test time : 65.1 Train 0.0 Validation 0.627902626991272 Test 0.6169185638427734
Epoch 4 train time : 392.9 loss: 1.29e-01
 test time : 69.5 Train 0.0 Validation 0.6248024702072144 Test 0.6132367849349976
Epoch 5 train time : 401.8 loss: 1.37e-01
 test time : 70.0 Train 0.0 Validation 0.6100293397903442 Test 0.5906917452812195
Epoch 6 train time : 379.7 loss: 1.47e-01
 test time : 65.2 Train 0.0 Validation 0.6229831576347351 Test 0.5933242440223694
Epoch 7 train time : 379.3 loss: 1.55e-01
 test time : 72.3 Train 0.0 Validation 0.5939509272575378 Test 0.591848611831665
Epoch 8 train time : 398.0 loss: 1.65e-01
 test time : 70.7 Train 0.0 Validation 0.6070548892021179 Test 0.5751053690910339
Epoch 9 train time : 389.4 loss: 1.69e-01
 test time : 65.0 Train 0.0 Validation 0.5813932418823242 Test 0.5724132061004639
Epoch 10 train time : 381.7 loss: 1.69e-01
 test time : 65.0 Train 0.0 Validation 0.6162999272346497 Test 0.597061038017273
Epoch 11 train time : 391.2 loss: 1.69e-01
 test time : 70.5 Train 0.0 Validation 0.6145750284194946 Test 0.5874685645103455
Epoch 12 train time : 398.7 loss: 1.69e-01
 test time : 68.1 Train 0.0 Validation 0.6064809560775757 Test 0.5849431753158569
Epoch 13 train time : 379.0 loss: 1.67e-01
 test time : 65.1 Train 0.0 Validation 0.6112381219863892 Test 0.5849776864051819
Epoch 14 train time : 383.3 loss: 1.67e-01
 test time : 70.6 Train 0.0 Validation 0.595410943031311 Test 0.5780882835388184
Epoch 15 train time : 397.1 loss: 1.67e-01
 test time : 70.3 Train 0.0 Validation 0.6119779348373413 Test 0.5844683647155762
Epoch 16 train time : 388.8 loss: 1.65e-01
 test time : 64.7 Train 0.0 Validation 0.5899442434310913 Test 0.5844772458076477
Epoch 17 train time : 381.8 loss: 1.64e-01
 test time : 65.3 Train 0.0 Validation 0.6033362746238708 Test 0.594292938709259
Epoch 18 train time : 395.0 loss: 1.65e-01
 test time : 71.0 Train 0.0 Validation 0.6247100830078125 Test 0.6061912178993225
Epoch 19 train time : 399.2 loss: 1.62e-01
 test time : 64.3 Train 0.0 Validation 0.5945572257041931 Test 0.57725590467453
Epoch 20 train time : 362.8 loss: 1.63e-01
 test time : 62.4 Train 0.0 Validation 0.6171067953109741 Test 0.5920530557632446
Epoch 21 train time : 362.1 loss: 1.61e-01
 test time : 62.3 Train 0.0 Validation 0.6149781346321106 Test 0.593672513961792
Epoch 22 train time : 362.7 loss: 1.61e-01
 test time : 62.4 Train 0.0 Validation 0.6120384931564331 Test 0.5911288261413574
Epoch 23 train time : 362.1 loss: 1.61e-01
 test time : 62.5 Train 0.0 Validation 0.6018850207328796 Test 0.5915478467941284
Epoch 24 train time : 360.4 loss: 1.59e-01
 test time : 62.4 Train 0.0 Validation 0.6135781407356262 Test 0.5891121625900269
Epoch 25 train time : 361.8 loss: 1.60e-01
 test time : 62.5 Train 0.0 Validation 0.5815764665603638 Test 0.5612159371376038
Epoch 26 train time : 363.6 loss: 1.60e-01
 test time : 62.8 Train 0.0 Validation 0.5822343826293945 Test 0.5938946604728699
Epoch 27 train time : 364.3 loss: 1.60e-01
 test time : 62.5 Train 0.0 Validation 0.6066351532936096 Test 0.5992982983589172
Epoch 28 train time : 362.7 loss: 1.57e-01
 test time : 62.7 Train 0.0 Validation 0.5942835211753845 Test 0.5966285467147827
Epoch 29 train time : 364.1 loss: 1.58e-01
 test time : 63.1 Train 0.0 Validation 0.6071633100509644 Test 0.5991935133934021
Epoch 30 train time : 384.2 loss: 1.56e-01
 test time : 62.8 Train 0.0 Validation 0.6167319416999817 Test 0.595421552658081
Epoch 31 train time : 367.1 loss: 1.56e-01
 test time : 63.0 Train 0.0 Validation 0.6019957065582275 Test 0.5973342657089233
Epoch 32 train time : 365.6 loss: 1.56e-01
 test time : 62.9 Train 0.0 Validation 0.5782500505447388 Test 0.5762178897857666
Epoch 33 train time : 365.5 loss: 1.55e-01
 test time : 63.5 Train 0.0 Validation 0.619358241558075 Test 0.611925482749939
Epoch 34 train time : 366.9 loss: 1.55e-01
 test time : 63.3 Train 0.0 Validation 0.6113160252571106 Test 0.6122851371765137
Epoch 35 train time : 367.7 loss: 1.54e-01
 test time : 63.2 Train 0.0 Validation 0.616767942905426 Test 0.5942283272743225
Epoch 36 train time : 368.5 loss: 1.53e-01
 test time : 63.1 Train 0.0 Validation 0.6174503564834595 Test 0.5928500890731812
Epoch 37 train time : 368.1 loss: 1.54e-01
 test time : 63.0 Train 0.0 Validation 0.6039528846740723 Test 0.5925389528274536
Epoch 38 train time : 369.9 loss: 1.53e-01
 test time : 63.2 Train 0.0 Validation 0.5870352387428284 Test 0.5777528882026672
Epoch 39 train time : 366.7 loss: 1.51e-01
 test time : 63.2 Train 0.0 Validation 0.6158271431922913 Test 0.6030524969100952
Epoch 40 train time : 370.2 loss: 1.51e-01
 test time : 63.2 Train 0.0 Validation 0.5801512002944946 Test 0.5636519193649292
Epoch 41 train time : 366.0 loss: 1.50e-01
 test time : 63.6 Train 0.0 Validation 0.6131208539009094 Test 0.6021337509155273
Epoch 42 train time : 367.6 loss: 1.50e-01
 test time : 63.2 Train 0.0 Validation 0.613440990447998 Test 0.5906621217727661
Epoch 43 train time : 367.5 loss: 1.52e-01
 test time : 63.1 Train 0.0 Validation 0.6041246652603149 Test 0.5645691156387329
Epoch 44 train time : 365.5 loss: 1.50e-01
 test time : 63.2 Train 0.0 Validation 0.6220774054527283 Test 0.6078558564186096
Epoch 45 train time : 369.0 loss: 1.50e-01
 test time : 63.1 Train 0.0 Validation 0.6016231775283813 Test 0.5894887447357178
Epoch 46 train time : 370.0 loss: 1.49e-01
 test time : 63.2 Train 0.0 Validation 0.5937084555625916 Test 0.5959941744804382
Epoch 47 train time : 367.2 loss: 1.48e-01
 test time : 63.1 Train 0.0 Validation 0.6056677103042603 Test 0.600097119808197
Epoch 48 train time : 364.7 loss: 1.48e-01
 test time : 63.1 Train 0.0 Validation 0.6011224389076233 Test 0.5957679748535156
Epoch 49 train time : 367.8 loss: 1.48e-01
 test time : 63.8 Train 0.0 Validation 0.6103379130363464 Test 0.608251690864563
Epoch 50 train time : 367.9 loss: 1.48e-01
 test time : 63.0 Train 0.0 Validation 0.5986627340316772 Test 0.56864994764328
Epoch 51 train time : 365.6 loss: 1.47e-01
 test time : 63.0 Train 0.0 Validation 0.61683189868927 Test 0.6000961661338806
Epoch 52 train time : 367.6 loss: 1.46e-01
 test time : 63.1 Train 0.0 Validation 0.6044603586196899 Test 0.593045711517334
Epoch 53 train time : 367.0 loss: 1.46e-01
 test time : 63.1 Train 0.0 Validation 0.6082334518432617 Test 0.5994857549667358
Epoch 54 train time : 369.2 loss: 1.46e-01
 test time : 63.1 Train 0.0 Validation 0.6206766963005066 Test 0.6158246397972107
Epoch 55 train time : 368.6 loss: 1.48e-01
 test time : 63.0 Train 0.0 Validation 0.6209269762039185 Test 0.609556257724762
Epoch 56 train time : 367.4 loss: 1.44e-01
 test time : 63.1 Train 0.0 Validation 0.6162533760070801 Test 0.5872120261192322
Epoch 57 train time : 362.9 loss: 1.46e-01
 test time : 63.6 Train 0.0 Validation 0.5978354215621948 Test 0.5863202214241028
Epoch 58 train time : 366.6 loss: 1.46e-01
 test time : 63.3 Train 0.0 Validation 0.6068606972694397 Test 0.5993455648422241
Epoch 59 train time : 364.2 loss: 1.45e-01
 test time : 63.0 Train 0.0 Validation 0.6062824726104736 Test 0.5946658849716187
Epoch 60 train time : 365.5 loss: 1.46e-01
 test time : 63.1 Train 0.0 Validation 0.6191104054450989 Test 0.6089785099029541
Epoch 61 train time : 368.0 loss: 1.46e-01
 test time : 63.0 Train 0.0 Validation 0.6109689474105835 Test 0.588248074054718
Epoch 62 train time : 366.5 loss: 1.44e-01
 test time : 63.1 Train 0.0 Validation 0.5896353721618652 Test 0.5786377787590027
Epoch 63 train time : 367.3 loss: 1.44e-01
 test time : 63.0 Train 0.0 Validation 0.6195675730705261 Test 0.6143608689308167
Epoch 64 train time : 366.8 loss: 1.42e-01
 test time : 63.0 Train 0.0 Validation 0.6051263213157654 Test 0.6088601350784302
Epoch 65 train time : 365.3 loss: 1.44e-01
 test time : 63.6 Train 0.0 Validation 0.6196972131729126 Test 0.6098576784133911
Epoch 66 train time : 366.2 loss: 1.41e-01
 test time : 63.0 Train 0.0 Validation 0.5906711220741272 Test 0.5859730839729309
Epoch 67 train time : 370.5 loss: 1.43e-01
 test time : 63.0 Train 0.0 Validation 0.6268810629844666 Test 0.5924839973449707
Epoch 68 train time : 364.1 loss: 1.43e-01
 test time : 63.0 Train 0.0 Validation 0.6142146587371826 Test 0.6089187860488892
Epoch 69 train time : 367.8 loss: 1.42e-01
 test time : 63.0 Train 0.0 Validation 0.6206754446029663 Test 0.6062567830085754
Epoch 70 train time : 366.1 loss: 1.42e-01
 test time : 63.0 Train 0.0 Validation 0.5852099657058716 Test 0.5749439597129822
Epoch 71 train time : 367.4 loss: 1.42e-01
 test time : 63.1 Train 0.0 Validation 0.6057137250900269 Test 0.5913661122322083
Epoch 72 train time : 367.0 loss: 1.42e-01
 test time : 63.0 Train 0.0 Validation 0.6027990579605103 Test 0.5970107316970825
Epoch 73 train time : 366.3 loss: 1.40e-01
 test time : 63.9 Train 0.0 Validation 0.6002171635627747 Test 0.5816455483436584
Epoch 74 train time : 365.2 loss: 1.36e-01
 test time : 63.0 Train 0.0 Validation 0.606168806552887 Test 0.6058277487754822
Epoch 75 train time : 365.0 loss: 1.24e-01
 test time : 63.0 Train 0.0 Validation 0.6152456998825073 Test 0.6181567907333374
Epoch 76 train time : 365.9 loss: 1.10e-01
 test time : 62.9 Train 0.0 Validation 0.634912371635437 Test 0.6198862791061401
Epoch 77 train time : 364.6 loss: 9.31e-02
 test time : 63.0 Train 0.0 Validation 0.6327410340309143 Test 0.6357256174087524
Epoch 78 train time : 367.8 loss: 7.68e-02
 test time : 63.0 Train 0.0 Validation 0.6333490014076233 Test 0.629121720790863
Epoch 79 train time : 366.6 loss: 6.54e-02
 test time : 62.9 Train 0.0 Validation 0.6329746842384338 Test 0.6326156854629517
Epoch 80 train time : 368.8 loss: 5.80e-02
 test time : 63.7 Train 0.0 Validation 0.6333740949630737 Test 0.6339271068572998
Best @0 validation score: 0.6365 Test score: 0.6310
[[0, tensor(0.6365), tensor(0.6310)]]
all runs:  0.0 0.6364809274673462 0.6310112476348877 0.0 0.0 0.0 
