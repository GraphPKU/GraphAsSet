/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
Namespace(dataset='pepstruct', repeat=1, num_workers=0, amp=True, compile=True, batch_size=6, testbatch_size=6, epochs=80, wd=0.1, lr=0.0003, beta=0.9, minlr=0.0, K=0.0, gradclipnorm=1.0, decompnoise=1e-06, seedoffset=0, warmstart=8, conststep=64, cosstep=8, use_y_scale=False, dp=0.0, eldp=0.0, act='silu', lossparam=0.0, advloss=False, embdp=0.0, embbn=False, emborthoinit=False, degreeemb=False, embln=True, featdim=-1, hiddim=88, caldim=-1, normA=False, laplacian=True, sqrtlambda=True, elres=True, usesvmix=True, vmean=True, vnorm=True, elvmean=True, elvnorm=True, snorm=True, gsizenorm=1.85, l_encoder='deepset', l_layers=3, l_combine='mul', l_aggr='mean', l_res=True, l_mlptailact1=True, l_mlplayers1=2, l_mlpnorm1='ln', l_mlptailact2=False, l_mlplayers2=0, l_mlpnorm2='none', num_layers=6, sv_uselinv=True, sv_tailact=True, sv_res=True, sv_numlayer=1, sv_norm='none', el_uselinv=True, el_uselins=False, el_tailact=True, el_numlayer=2, el_norm='none', el_uses=False, conv_uselinv=True, conv_tailact=True, conv_numlayer=1, conv_norm='none', predlin_numlayer=1, predlin_norm='none', lexp='mlp', lexp_layer=2, lexp_norm='ln', outln=False, pool='mean', Tm=1, save=None, load='pepstruct.wd1e-1', use_pos=False, align_size=32)
fixed l1reg
10873 2331 2331
split 10873 2331 2331
num_task 11
PiOModel(
  (inputencoder): QInputEncoder(
    (xemb): MultiEmbedding(
      (embedding_list): ModuleList(
        (0): Embedding(18, 88, padding_idx=0)
        (1): Embedding(4, 88, padding_idx=0)
        (2-3): 2 x Embedding(8, 88, padding_idx=0)
        (4): Embedding(6, 88, padding_idx=0)
        (5): Embedding(2, 88, padding_idx=0)
        (6): Embedding(7, 88, padding_idx=0)
        (7-8): 2 x Embedding(3, 88, padding_idx=0)
      )
      (postemb): Sequential(
        (0): LayerNorm((88,), eps=1e-05, elementwise_affine=False)
      )
    )
    (edgeEmb): MultiEmbedding(
      (embedding_list): ModuleList(
        (0-2): 3 x Embedding(5, 88, padding_idx=0)
      )
      (postemb): Sequential(
        (0): LayerNorm((88,), eps=1e-05, elementwise_affine=False)
      )
    )
    (LambdaEmb): MLPEncoding(
      (lin): MLP(
        (lin): Sequential(
          (0): Linear(in_features=1, out_features=176, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((176,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=176, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
  )
  (LambdaEncoder): PermEquiLayer(
    (set2set): Sequential(
      (0): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (1): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (2): Set2Set(
      (mlp1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
      (mlp2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): LayerNorm(
            (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)
          )
          (5): SiLU(inplace=True)
        )
      )
    )
      (3): Identity()
    )
    (set2vec): Sequential(
      (0): MLP(
      (lin): Sequential(
        (0): NoneNorm()
      )
    )
    )
  )
  (elprojs): ModuleList(
    (0-5): 6 x sv2el(
      (linv1): Linear(in_features=88, out_features=88, bias=False)
      (linv2): Linear(in_features=88, out_features=88, bias=False)
      (lins1): Identity()
      (lins2): Identity()
      (lin): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
          (3): Linear(in_features=88, out_features=88, bias=True)
          (4): NoneNorm()
          (5): SiLU(inplace=True)
        )
      )
    )
  )
  (svmixs): ModuleList(
    (0-5): 6 x svMix(
      (linv1): Linear(in_features=88, out_features=88, bias=False)
      (linv2): Linear(in_features=88, out_features=88, bias=False)
      (linv3): Identity()
      (lins1): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (lins2): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (lins3): Identity()
    )
  )
  (convs): ModuleList(
    (0-5): 6 x DirCFConv(
      (lins): MLP(
        (lin): Sequential(
          (0): Linear(in_features=88, out_features=88, bias=True)
          (1): NoneNorm()
          (2): SiLU(inplace=True)
        )
      )
      (linv): Linear(in_features=88, out_features=88, bias=False)
    )
  )
  (predlin): MLP(
    (lin): Sequential(
      (0): Linear(in_features=88, out_features=11, bias=True)
    )
  )
  (predln): Identity()
  (vln): Sequential(
    (0): VMean()
    (1): VNorm()
  )
  (elvln): Sequential(
    (0): VMean()
    (1): VNorm()
  )
  (sln): LayerNorm((88,), eps=1e-05, elementwise_affine=False)
)
numel 587323
mod/pepstruct.wd1e-1.0.pt
<All keys matched successfully>
None
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Epoch 1 train time : 1089.2 loss: 2.44e-01
 test time : 438.1 Train 0.0 Validation 0.2561812102794647 Test 0.2635020315647125
GPU memory 19.32
Epoch 2 train time : 411.5 loss: 2.47e-01
 test time : 65.2 Train 0.0 Validation 0.256320595741272 Test 0.2594999670982361
Epoch 3 train time : 387.0 loss: 2.50e-01
 test time : 67.7 Train 0.0 Validation 0.25837767124176025 Test 0.26552584767341614
Epoch 4 train time : 394.6 loss: 2.53e-01
 test time : 71.5 Train 0.0 Validation 0.25531068444252014 Test 0.2646258771419525
Epoch 5 train time : 417.7 loss: 2.58e-01
 test time : 75.4 Train 0.0 Validation 0.272311270236969 Test 0.2797417938709259
Epoch 6 train time : 510.5 loss: 2.61e-01
 test time : 65.7 Train 0.0 Validation 0.2715596854686737 Test 0.2781294882297516
Epoch 7 train time : 397.2 loss: 2.64e-01
 test time : 71.4 Train 0.0 Validation 0.2597239315509796 Test 0.2668614983558655
Epoch 8 train time : 429.6 loss: 2.68e-01
 test time : 75.7 Train 0.0 Validation 0.2604607343673706 Test 0.26710954308509827
Epoch 9 train time : 399.9 loss: 2.69e-01
 test time : 67.4 Train 0.0 Validation 0.27561429142951965 Test 0.27649566531181335
Epoch 10 train time : 391.5 loss: 2.71e-01
 test time : 71.9 Train 0.0 Validation 0.2666836678981781 Test 0.26967179775238037
Epoch 11 train time : 425.6 loss: 2.69e-01
 test time : 76.0 Train 0.0 Validation 0.26434969902038574 Test 0.2704428732395172
Epoch 12 train time : 405.8 loss: 2.70e-01
 test time : 68.1 Train 0.0 Validation 0.26629316806793213 Test 0.27189740538597107
Epoch 13 train time : 392.3 loss: 2.72e-01
 test time : 68.6 Train 0.0 Validation 0.2679107189178467 Test 0.271269291639328
Epoch 14 train time : 415.7 loss: 2.72e-01
 test time : 77.2 Train 0.0 Validation 0.26727378368377686 Test 0.2735559046268463
Epoch 15 train time : 421.7 loss: 2.70e-01
 test time : 65.2 Train 0.0 Validation 0.2696641981601715 Test 0.27900388836860657
Epoch 16 train time : 392.5 loss: 2.68e-01
 test time : 67.5 Train 0.0 Validation 0.27167627215385437 Test 0.2752014398574829
Epoch 17 train time : 405.8 loss: 2.69e-01
 test time : 76.4 Train 0.0 Validation 0.27441105246543884 Test 0.2769589424133301
Epoch 18 train time : 422.6 loss: 2.69e-01
 test time : 76.0 Train 0.0 Validation 0.278782457113266 Test 0.28582563996315
Epoch 19 train time : 377.1 loss: 2.68e-01
 test time : 62.8 Train 0.0 Validation 0.2926808297634125 Test 0.30953463912010193
Epoch 20 train time : 362.2 loss: 2.69e-01
 test time : 62.7 Train 0.0 Validation 0.2786472737789154 Test 0.2903296947479248
Epoch 21 train time : 364.2 loss: 2.69e-01
 test time : 62.9 Train 0.0 Validation 0.28045332431793213 Test 0.287244588136673
Epoch 22 train time : 367.3 loss: 2.71e-01
 test time : 63.1 Train 0.0 Validation 0.2700524628162384 Test 0.2801647186279297
Epoch 23 train time : 364.9 loss: 2.69e-01
 test time : 62.8 Train 0.0 Validation 0.26159894466400146 Test 0.2661888897418976
Epoch 24 train time : 363.0 loss: 2.67e-01
 test time : 62.9 Train 0.0 Validation 0.26845526695251465 Test 0.2699476182460785
Epoch 25 train time : 364.9 loss: 2.68e-01
 test time : 62.8 Train 0.0 Validation 0.26249468326568604 Test 0.2720562815666199
Epoch 26 train time : 363.7 loss: 2.69e-01
 test time : 62.8 Train 0.0 Validation 0.26146405935287476 Test 0.2732705771923065
Epoch 27 train time : 364.6 loss: 2.66e-01
 test time : 62.9 Train 0.0 Validation 0.2707234025001526 Test 0.27938124537467957
Epoch 28 train time : 375.2 loss: 2.66e-01
 test time : 63.7 Train 0.0 Validation 0.2740646302700043 Test 0.28041699528694153
Epoch 29 train time : 375.1 loss: 2.69e-01
 test time : 64.0 Train 0.0 Validation 0.27291277050971985 Test 0.27649030089378357
Epoch 30 train time : 375.3 loss: 2.68e-01
 test time : 64.1 Train 0.0 Validation 0.2665582597255707 Test 0.26888853311538696
Epoch 31 train time : 378.6 loss: 2.64e-01
 test time : 63.9 Train 0.0 Validation 0.27732422947883606 Test 0.28412920236587524
Epoch 32 train time : 376.4 loss: 2.68e-01
 test time : 63.8 Train 0.0 Validation 0.2598457634449005 Test 0.2661927044391632
Epoch 33 train time : 378.3 loss: 2.65e-01
 test time : 64.6 Train 0.0 Validation 0.2631678879261017 Test 0.27117300033569336
Epoch 34 train time : 379.1 loss: 2.67e-01
 test time : 64.3 Train 0.0 Validation 0.2605697810649872 Test 0.26637008786201477
Epoch 35 train time : 377.8 loss: 2.66e-01
 test time : 64.3 Train 0.0 Validation 0.30314698815345764 Test 0.31245625019073486
Epoch 36 train time : 378.4 loss: 2.65e-01
 test time : 64.5 Train 0.0 Validation 0.26563090085983276 Test 0.27250000834465027
Epoch 37 train time : 377.8 loss: 2.65e-01
 test time : 64.7 Train 0.0 Validation 0.2682111859321594 Test 0.27404728531837463
Epoch 38 train time : 378.2 loss: 2.65e-01
 test time : 64.5 Train 0.0 Validation 0.26021963357925415 Test 0.2652669847011566
Epoch 39 train time : 377.1 loss: 2.65e-01
 test time : 64.3 Train 0.0 Validation 0.27683335542678833 Test 0.2858199179172516
Epoch 40 train time : 377.2 loss: 2.66e-01
 test time : 64.4 Train 0.0 Validation 0.289516419172287 Test 0.3033597767353058
Epoch 41 train time : 377.2 loss: 2.66e-01
 test time : 64.6 Train 0.0 Validation 0.2649577260017395 Test 0.27339375019073486
Epoch 42 train time : 378.8 loss: 2.64e-01
 test time : 64.3 Train 0.0 Validation 0.2693544626235962 Test 0.2816806733608246
Epoch 43 train time : 378.5 loss: 2.64e-01
 test time : 64.4 Train 0.0 Validation 0.26025304198265076 Test 0.2645106911659241
Epoch 44 train time : 374.5 loss: 2.67e-01
 test time : 64.2 Train 0.0 Validation 0.26906341314315796 Test 0.27599969506263733
Epoch 45 train time : 378.0 loss: 2.65e-01
 test time : 64.5 Train 0.0 Validation 0.26799437403678894 Test 0.2714177370071411
Epoch 46 train time : 378.0 loss: 2.65e-01
 test time : 64.6 Train 0.0 Validation 0.2715816795825958 Test 0.2798815071582794
Epoch 47 train time : 381.1 loss: 2.64e-01
 test time : 64.5 Train 0.0 Validation 0.269387423992157 Test 0.27590006589889526
Epoch 48 train time : 376.5 loss: 2.65e-01
 test time : 64.2 Train 0.0 Validation 0.2698058784008026 Test 0.27294304966926575
Epoch 49 train time : 377.5 loss: 2.63e-01
 test time : 64.2 Train 0.0 Validation 0.2634989023208618 Test 0.26576340198516846
Epoch 50 train time : 376.0 loss: 2.65e-01
 test time : 64.6 Train 0.0 Validation 0.26368895173072815 Test 0.26777753233909607
Epoch 51 train time : 378.2 loss: 2.66e-01
 test time : 64.4 Train 0.0 Validation 0.26665198802948 Test 0.2688148617744446
Epoch 52 train time : 377.0 loss: 2.64e-01
 test time : 63.8 Train 0.0 Validation 0.2842184901237488 Test 0.2867536246776581
Epoch 53 train time : 377.1 loss: 2.63e-01
 test time : 64.4 Train 0.0 Validation 0.3227880597114563 Test 0.3340897858142853
Epoch 54 train time : 378.6 loss: 2.64e-01
 test time : 64.1 Train 0.0 Validation 0.2628193497657776 Test 0.2676687240600586
Epoch 55 train time : 378.1 loss: 2.66e-01
 test time : 64.4 Train 0.0 Validation 0.2702110707759857 Test 0.2762155532836914
Epoch 56 train time : 375.4 loss: 2.63e-01
 test time : 64.3 Train 0.0 Validation 0.2586345374584198 Test 0.2650388479232788
Epoch 57 train time : 377.5 loss: 2.64e-01
 test time : 64.4 Train 0.0 Validation 0.26294568181037903 Test 0.26912903785705566
Epoch 58 train time : 380.1 loss: 2.62e-01
 test time : 64.7 Train 0.0 Validation 0.26408445835113525 Test 0.2718541920185089
Epoch 59 train time : 379.0 loss: 2.62e-01
 test time : 64.3 Train 0.0 Validation 0.2660595178604126 Test 0.26903674006462097
Epoch 60 train time : 379.7 loss: 2.63e-01
 test time : 64.3 Train 0.0 Validation 0.26850631833076477 Test 0.28068020939826965
Epoch 61 train time : 377.3 loss: 2.64e-01
 test time : 64.4 Train 0.0 Validation 0.259991317987442 Test 0.2666199207305908
Epoch 62 train time : 374.5 loss: 2.62e-01
 test time : 64.4 Train 0.0 Validation 0.2557997405529022 Test 0.26712387800216675
Epoch 63 train time : 376.5 loss: 2.63e-01
 test time : 64.6 Train 0.0 Validation 0.2575104832649231 Test 0.26407623291015625
Epoch 64 train time : 376.9 loss: 2.63e-01
 test time : 64.7 Train 0.0 Validation 0.26857486367225647 Test 0.2713770568370819
Epoch 65 train time : 374.8 loss: 2.61e-01
 test time : 64.3 Train 0.0 Validation 0.2618825137615204 Test 0.27083995938301086
Epoch 66 train time : 379.1 loss: 2.63e-01
 test time : 64.0 Train 0.0 Validation 0.27067676186561584 Test 0.28018754720687866
Epoch 67 train time : 377.1 loss: 2.62e-01
 test time : 64.3 Train 0.0 Validation 0.26354214549064636 Test 0.27284616231918335
Epoch 68 train time : 377.5 loss: 2.61e-01
 test time : 64.5 Train 0.0 Validation 0.27240052819252014 Test 0.28310784697532654
Epoch 69 train time : 378.6 loss: 2.61e-01
 test time : 64.3 Train 0.0 Validation 0.2666526734828949 Test 0.27334991097450256
Epoch 70 train time : 377.0 loss: 2.62e-01
 test time : 64.3 Train 0.0 Validation 0.25793591141700745 Test 0.2699822783470154
Epoch 71 train time : 376.9 loss: 2.62e-01
 test time : 64.5 Train 0.0 Validation 0.25914251804351807 Test 0.2674275040626526
Epoch 72 train time : 377.7 loss: 2.61e-01
 test time : 64.4 Train 0.0 Validation 0.26335421204566956 Test 0.2727445065975189
Epoch 73 train time : 376.9 loss: 2.60e-01
 test time : 64.5 Train 0.0 Validation 0.2635335624217987 Test 0.26797014474868774
Epoch 74 train time : 374.6 loss: 2.59e-01
 test time : 64.4 Train 0.0 Validation 0.26784548163414 Test 0.27335625886917114
Epoch 75 train time : 376.1 loss: 2.55e-01
 test time : 64.4 Train 0.0 Validation 0.26096075773239136 Test 0.2636958658695221
Epoch 76 train time : 379.1 loss: 2.49e-01
 test time : 64.2 Train 0.0 Validation 0.26673081517219543 Test 0.27087798714637756
Epoch 77 train time : 374.7 loss: 2.43e-01
 test time : 64.7 Train 0.0 Validation 0.2531309127807617 Test 0.25739768147468567
Epoch 78 train time : 379.1 loss: 2.36e-01
 test time : 64.6 Train 0.0 Validation 0.24974480271339417 Test 0.2581811845302582
Epoch 79 train time : 376.2 loss: 2.31e-01
 test time : 64.3 Train 0.0 Validation 0.250805526971817 Test 0.2562216818332672
Epoch 80 train time : 375.2 loss: 2.27e-01
 test time : 64.4 Train 0.0 Validation 0.25007787346839905 Test 0.2573109269142151
Best @77 validation score: 0.2497 Test score: 0.2582
[[77, tensor(0.2497), tensor(0.2582)]]
all runs:  77.0 0.24974480271339417 0.2581811845302582 0.0 0.0 0.0 
