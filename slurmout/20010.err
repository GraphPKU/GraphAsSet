/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
Traceback (most recent call last):
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main.py", line 492, in <module>
    main()
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main.py", line 438, in main
    loss = train(lossfn, model, device, train_loader,
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main.py", line 53, in train
    finalpred = model(*datatuple)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 82, in forward
    return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/PiOModel.py", line 203, in forward
Traceback (most recent call last):
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main.py", line 492, in <module>
    def forward(self, A, X, nodemask, *inputtuple):
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    main()
    return fn(*args, **kwargs)
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main.py", line 438, in main
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2836, in forward
    loss = train(lossfn, model, device, train_loader,
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main.py", line 53, in train
    finalpred = model(*datatuple)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 82, in forward
    return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)
    return compiled_fn(full_args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return fn(*args, **kwargs)
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/PiOModel.py", line 203, in forward
    def forward(self, A, X, nodemask, *inputtuple):
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return f(*args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2403, in debug_compiled_function
    return fn(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2836, in forward
    return compiled_function(*args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1900, in runtime_wrapper
    return compiled_fn(full_args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    all_outs = call_func_with_args(
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    return f(*args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2403, in debug_compiled_function
    out = normalize_as_list(f(args))
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
    return compiled_function(*args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1900, in runtime_wrapper
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2168, in forward
    all_outs = call_func_with_args(
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    fw_outs = call_func_with_args(
    out = normalize_as_list(f(args))
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    out = normalize_as_list(f(args))
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 248, in run
    return f(*args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2168, in forward
    return model(new_inputs)
  File "/tmp/torchinductor_muhan01/yy/cyyqama7ifjr3olmv24f5yulpkied75evseceffdyvhbkug6tgdz.py", line 4060, in call
    buf586 = empty_strided((663552, 80), (80, 1), device='cuda', dtype=torch.float16)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 0; 23.69 GiB total capacity; 10.88 GiB already allocated; 102.94 MiB free; 11.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    fw_outs = call_func_with_args(
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 248, in run
    return model(new_inputs)
  File "/tmp/torchinductor_muhan01/yy/cyyqama7ifjr3olmv24f5yulpkied75evseceffdyvhbkug6tgdz.py", line 3892, in call
    triton__53.run(buf339, buf396, buf453, buf510, buf512, 645120, 80, grid=grid(645120), stream=stream0)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/triton_ops/autotune.py", line 182, in run
    self.autotune_to_one_config(*args, grid=grid)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper
    r = func(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/triton_ops/autotune.py", line 169, in autotune_to_one_config
    timings = {
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/triton_ops/autotune.py", line 170, in <dictcomp>
    launcher: self.bench(launcher, *cloned_args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/triton_ops/autotune.py", line 151, in bench
    return do_bench(kernel_call, rep=40, fast_flush=True)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/triton/testing.py", line 162, in do_bench
    cache = torch.empty(int(256e6 // 4), dtype=torch.int, device='cuda')
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 23.69 GiB total capacity; 9.29 GiB already allocated; 102.94 MiB free; 9.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
