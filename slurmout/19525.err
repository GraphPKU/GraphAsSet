/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Traceback (most recent call last):
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main_deepset.py", line 512, in <module>
    main()
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main_deepset.py", line 458, in main
    loss = train(lossfn, model, device, train_loader,
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main_deepset.py", line 53, in train
    finalpred = model(*datatuple)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 82, in forward
    return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/ONDeepSet.py", line 184, in forward
    def forward(self, A, X, nodemask, *inputtuple):
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2836, in forward
    return compiled_fn(full_args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2403, in debug_compiled_function
    return compiled_function(*args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1900, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2168, in forward
    fw_outs = call_func_with_args(
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 248, in run
    return model(new_inputs)
  File "/tmp/torchinductor_muhan01/ix/cix5ieklaxgkptmuq7v37brbark3fzf2v3djdfwwofucgjhw62bm.py", line 3298, in call
    triton__0.run(primals_343, buf0, 589824, grid=grid(589824), stream=stream0)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/triton_ops/autotune.py", line 190, in run
    result = launcher(
  File "<string>", line 6, in launcher
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/triton/compiler.py", line 1678, in __getattribute__
    self._init_handles()
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/triton/compiler.py", line 1668, in _init_handles
    max_shared = cuda_utils.get_device_properties(device)["max_shared_mem"]
RuntimeError: Triton Error [CUDA]: unknown error
Traceback (most recent call last):
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main_deepset.py", line 512, in <module>
    main()
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main_deepset.py", line 466, in main
    valid_perf = eval(model, device, valid_loader, evaluator, args.amp, args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main_deepset.py", line 101, in eval
    tpred = model(*datatuple)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 82, in forward
    return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/ONDeepSet.py", line 184, in forward
    def forward(self, A, X, nodemask, *inputtuple):
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2836, in forward
    return compiled_fn(full_args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1224, in g
    return f(*args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1900, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 248, in run
    return model(new_inputs)
  File "/tmp/torchinductor_muhan01/vt/cvtfeltt6fhzsd6zjtta2jmiomsf4q3iucyddglnpc4xdvsu72zv.py", line 3625, in call
    triton__3.run(buf3, arg342_1, buf4, 160000, grid=grid(160000), stream=stream0)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/triton_ops/autotune.py", line 190, in run
    result = launcher(
  File "<string>", line 6, in launcher
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/triton/compiler.py", line 1678, in __getattribute__
    self._init_handles()
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/triton/compiler.py", line 1668, in _init_handles
    max_shared = cuda_utils.get_device_properties(device)["max_shared_mem"]
RuntimeError: Triton Error [CUDA]: unknown error
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
slurmstepd: error: *** JOB 19525 ON node001 CANCELLED AT 2023-11-13T21:32:14 ***
slurmstepd: error: *** JOB 19525 STEPD TERMINATED ON node001 AT 2023-11-13T21:33:14 DUE TO JOB NOT ENDING WITH SIGNALS ***
slurmstepd: error: Unable to destroy container 800453 in cgroup plugin, giving up after 63 sec
