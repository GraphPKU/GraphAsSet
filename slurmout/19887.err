/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
/ceph/home/muhan01/wangxiyuan/GraphAsSet/utils.py:20: UserWarning: not equivalent to Identity
  warnings.warn("not equivalent to Identity")
Traceback (most recent call last):
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main.py", line 492, in <module>
    main()
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main.py", line 438, in main
    loss = train(lossfn, model, device, train_loader,
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main.py", line 65, in train
    ampscaler.scale(value_loss).backward()
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/autograd/function.py", line 274, in apply
    return user_fn(self, *args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2365, in backward
    out = call_compiled_backward()
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2341, in call_compiled_backward
    out = call_func_with_args(
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 248, in run
    return model(new_inputs)
  File "/tmp/torchinductor_muhan01/ck/cck3e35ycazwsfzqihl7t2kwq7f5bejot4eo4qcmaku6y3ysw6xo.py", line 4359, in call
    buf81 = empty_strided((432, 447, 448), (200256, 448, 1), device='cuda', dtype=torch.float16)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.69 GiB total capacity; 19.39 GiB already allocated; 110.94 MiB free; 22.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main.py", line 492, in <module>
    main()
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main.py", line 438, in main
    loss = train(lossfn, model, device, train_loader,
  File "/ceph/home/muhan01/wangxiyuan/GraphAsSet/main.py", line 65, in train
    ampscaler.scale(value_loss).backward()
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/autograd/function.py", line 274, in apply
    return user_fn(self, *args)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2365, in backward
    out = call_compiled_backward()
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 2341, in call_compiled_backward
    out = call_func_with_args(
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1249, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 248, in run
    return model(new_inputs)
  File "/tmp/torchinductor_muhan01/lu/cluqevp7djamgq5epre2frc57uk3io6whju36jgfruevbnkxj5ul.py", line 4377, in call
    triton__11.run(buf50, buf54, sub_42, pow_31, buf85, buf89, buf56, buf91, 193536, 447, grid=grid(193536), stream=stream0)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/triton_ops/autotune.py", line 182, in run
    self.autotune_to_one_config(*args, grid=grid)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 163, in time_wrapper
    r = func(*args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/triton_ops/autotune.py", line 169, in autotune_to_one_config
    timings = {
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/triton_ops/autotune.py", line 170, in <dictcomp>
    launcher: self.bench(launcher, *cloned_args, **kwargs)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/_inductor/triton_ops/autotune.py", line 151, in bench
    return do_bench(kernel_call, rep=40, fast_flush=True)
  File "/ceph/home/muhan01/miniconda3/envs/torch2/lib/python3.10/site-packages/triton/testing.py", line 162, in do_bench
    cache = torch.empty(int(256e6 // 4), dtype=torch.int, device='cuda')
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 23.69 GiB total capacity; 19.72 GiB already allocated; 102.94 MiB free; 22.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
